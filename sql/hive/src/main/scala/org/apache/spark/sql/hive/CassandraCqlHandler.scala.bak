package org.apache.spark.sql.hive

import java.util

import com.datastax.driver.core.KeyspaceMetadata
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hive.metastore.HiveMetaHook
import org.apache.hadoop.hive.metastore.api.Table
import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler
import org.apache.hadoop.hive.ql.plan.TableDesc
import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider
import org.apache.hadoop.hive.serde2.SerDe
import org.apache.hadoop.mapred.{InputFormat, JobConf, OutputFormat}
import org.apache.spark.sql.CassandraSchemaHelper

import scala.collection.JavaConversions._

object CassandraHiveConstants {
  val CASSANDRA_HOST = "cassandra.host";
  // initialHost
  val CASSANDRA_RPC_PORT = "cassandra.port.rpc";
  // rcpPort
  val CASSANDRA_NATIVE_PORT = "cassandra.port.native";
  // rcpPort
  val CASSANDRA_KEYSPACE_NAME = "cassandra.ks.name";
  // keyspace
  val CASSANDRA_KEYSPACE_REPFACTOR = "cassandra.ks.repfactor";
  //keyspace replication factor
  val CASSANDRA_CF_NAME = "cassandra.cf.name";
  // column family
  val CASSANDRA_USERNAME = "cassandra.username"
  val CASSANDRA_PASSWORD = "cassandra.password"

  val defaultHost = "127.0.0.1"
  val defaultRpcPort = "9160"
  val defaultNativePort = "9042"
}

case class CassandraProperties(properties: Map[String, String], configuration: Configuration) {
  lazy val cassandraHost = properties.get(CassandraHiveConstants.CASSANDRA_HOST) match {
    case Some(host) => host
    case None =>
      if (configuration != null) {
        configuration.get(CassandraHiveConstants.CASSANDRA_HOST, CassandraHiveConstants.defaultHost)
      } else {
        CassandraHiveConstants.defaultHost
      }
  }

  lazy val cassandraRpcPort = properties.get(CassandraHiveConstants.CASSANDRA_NATIVE_PORT) match {
    case Some(rpcPort) => rpcPort
    case None =>
      if (configuration != null) {
        configuration.get(CassandraHiveConstants.CASSANDRA_RPC_PORT, CassandraHiveConstants.defaultRpcPort)
      } else {
        CassandraHiveConstants.defaultRpcPort
      }
  }

  lazy val cassandraNativePort = properties.get(CassandraHiveConstants.CASSANDRA_NATIVE_PORT) match {
    case Some(nativePort) => nativePort
    case None =>
      if (configuration != null) {
        configuration.get(CassandraHiveConstants.CASSANDRA_NATIVE_PORT, CassandraHiveConstants.defaultNativePort)
      } else {
        CassandraHiveConstants.defaultNativePort
      }
  }

  lazy val cassandraUsername: Option[String] = properties.get(CassandraHiveConstants.CASSANDRA_USERNAME) match {
    case Some(user) => Some(user)
    case None =>
      if (configuration != null) {
        val user = configuration.get(CassandraHiveConstants.CASSANDRA_USERNAME, "")
        if (user.length == 0) {
          None
        } else {
          Some(user)
        }
      } else {
        None
      }
  }

  lazy val cassandraPassword: Option[String] = properties.get(CassandraHiveConstants.CASSANDRA_PASSWORD) match {
    case Some(password) => Some(password)
    case None =>
      if (configuration != null) {
        val password = configuration.get(CassandraHiveConstants.CASSANDRA_PASSWORD, "")
        if (password.length == 0) {
          None
        } else {
          Some(password)
        }
      } else {
        None
      }
  }
}

class CassandraCqlHandler extends HiveStorageHandler {
  var configuration: Configuration = _

  override def getMetaHook: HiveMetaHook = {
    new HiveMetaHook {
      override def preCreateTable(table: Table): Unit = {
        val dbName = table.getDbName
        val tableName = table.getTableName

        val properties: Map[String, String] = (table.getSd.getSerdeInfo.getParameters ++ table.getParameters).toMap

        val cassandraProperties = CassandraProperties(properties, configuration)

        val metaData = CassandraSchemaHelper.getCassandraMetadata(cassandraProperties.cassandraHost,
          cassandraProperties.cassandraNativePort,
          cassandraProperties.cassandraUsername,
          cassandraProperties.cassandraPassword)


        val keyspace: KeyspaceMetadata = metaData.getKeyspace(dbName)
        if (keyspace == null) {
          //TODO: Create keyspace
          throw new RuntimeException(s"Keyspace [$dbName] does not exist")
        }

        val cf = keyspace.getTable(tableName)
        if (cf == null) {
          //TODO: Create keyspace
          throw new RuntimeException(s"Table [$table] does not exist")
        }
      }

      override def rollbackDropTable(p1: Table): Unit = {}

      override def commitDropTable(p1: Table, p2: Boolean): Unit = {}

      override def preDropTable(p1: Table): Unit = {}

      override def commitCreateTable(p1: Table): Unit = {}

      override def rollbackCreateTable(p1: Table): Unit = {}
    }
  }

  override def getInputFormatClass: Class[_ <: InputFormat[_, _]] = {
    //THIS WILL NEVER BE USED
    null
  }

  override def getOutputFormatClass: Class[_ <: OutputFormat[_, _]] = {
    null
  }

  override def configureInputJobProperties(p1: TableDesc, p2: util.Map[String, String]): Unit = {}

  override def configureTableJobProperties(p1: TableDesc, p2: util.Map[String, String]): Unit = {}

  override def configureOutputJobProperties(p1: TableDesc, p2: util.Map[String, String]): Unit = {}

  override def configureJobConf(p1: TableDesc, p2: JobConf): Unit = {}

  override def getSerDeClass: Class[_ <: SerDe] = {
    null
  }

  override def getAuthorizationProvider: HiveAuthorizationProvider = {
    null
  }

  override def getConf: Configuration = configuration

  override def setConf(conf: Configuration): Unit = {
    configuration = conf
  }
}
